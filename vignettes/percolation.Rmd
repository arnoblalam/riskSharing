---
title: "Percolation on Risk Sharing network"
author: "Arnob Alam"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

This vignette simulates site percolation in our risk sharing network.  _Site
percolation_ refers to the random removal of nodes on the network.  In our
network this could be due to migration, death, or agents simply opting out of
their obligations (default).  We are interested in whether the network is
connected (that is, a giant component exists) after a fraction $\phi_c$ of the
nodes are randomly removed.

# Algorithm

We use the following algorithms to simulate percolation (our graph has 119
vertices):

1. Start with n = 1, m = 1, i = 0, sim_size = 1000
2. Randomly remove n out of 119 vertices from the graph
3. Calculate the size of the relative size of the largest connected component
(number of nodes in largest component/119)
4. Store the results of step 4 in i
5. Increase m by 1
6. If m < sim_size, go back to step 2
7. Calculate $S$ the average size of giant component as $S = \frac{i}{sim\_size}$
8. Increse n by 1
9. If n < 118, return to step 2

# Results

We implment the algorithm below. First we load the data and create the graph

```{r load-data}
# Load package
library(riskSharing)

#Get data
data("nyakatoke")

# Create the underreporting graph

# For the underreporting model, we create a graph from the dataframe
# First we keep only the rows where at least one houshold said the other is in
# their network.
underreporting.df <- 
    nyakatoke[nyakatoke$willingness_link1 == 1 | nyakatoke$willingness_link2 == 1,]
# We then convert this into an undirected graph
g.underreporting <- igraph::graph_from_data_frame(underreporting.df, 
                                                  directed = FALSE)
# We then remove any multiple edges
g.underreporting <- igraph::simplify(g.underreporting)
```

We then implement the algorithm described:

```{r percolation}
S <- numeric(120) # vector to hold our results
sim_size <- 1000 # Number of simulations
i <- numeric(sim_size) # Vector to hold cluster sizes
for (n in 0:119) {
    # n is the number of nodes to remove
    for (m in 1:sim_size) {
        # m is the mth simulation
        to_remove <- sample(1:119, n)
        # randomly pick n of the 119 nodes to remove
        i[m] <- igraph::components(
            igraph::delete_vertices(g.underreporting, to_remove))$csize[1]/119
        # calculate the size of the giant component and save the result to
        # the variable i
    }
    S[n + 1] <- mean(i)
    # the average cluster size (after removing n nodes) is the average of the 
    # i variable
}
```

Let's put the results in a nice table/graph.  We will also overlay theoretical
results for the size of the giant component for the Poisson random graph and the
Power law graph.  These theoretical results are obrained by using the generating
function framework described in (Newman Date).  The theoretical size of the
giant component is given by $S=\phi[1-g_0(u)]$, where 
$g_0(u)=\sum_{k=0}^\infty(p_k z^k)$.  We have to calculate the value of $u$,
which is the average probability that a vertex is not connected to the giant
component via a particular neighboring vertex.  This is given by 
$u = 1 - \phi+\phi g_1(u)$ where $g_1(z) = \sum_{k=0}^\infty(q_k z^k)$,
$q_k = \frac{(k+1) p_{k+1}}{<k>}$ and $<k>$ is the average degree of the network.

For the Possion case, the appropriate generating functions are:
$$
g_0(z)=e^{\lambda(z-1)}\\
g_1(z)=e^{\lambda(z-1)}
$$
where $\lambda$ is the average degree of the graph (same as $<k>$).  We first
solve for $u$ (using the equation $u = 1 - \phi+\phi g_1(u)$) and then
subsitute the results back into $S=\phi[1-g_0(u)]$.  For the Poisson case, we
obtain:

$$
u = 1 - \phi - \frac{W(\lambda \phi-e^{-\lambda \phi})}{\lambda}\\
S = \frac{W(-\lambda \phi e^{-\lambda \phi})}{\lambda}
$$

For the power law case, we obtain

$$
g_0(z) = \frac{Li_\alpha(z)}{\zeta(\alpha)} \\
g_1(z) = \frac{Li_{\alpha - 1}(z)}{z \zeta(\alpha - 1)}
$$
Unfortunately, we are not unable to solve the above explicitly for $u(\phi)$.
We therefore simulate percolation on a pure power law graph with the same
power law exponent as the observed graph using the simulation algorithm above.

```{r phi v S underrepprting, fig.width=6, fig.height=4}
my_df <- data.frame(phi = 1 - (0:119)/119, S = S)
# rbind(my_df, c(phi = 0, S = 1), c(phi = 1, S = 0))
main = expression(paste("Occupaion Probability, ", phi, ", vs. size of giant cluster ", S))
plot(my_df[1:120,], 
     type = "l", 
     main = main,
     xlab = expression(phi),
     xlim = c(1,0))
# We will also add the theoretical results - first exponential
lambda <- mean(igraph::degree(g.underreporting))
plot(function(phi) phi + gsl::lambert_W0(-lambda * phi * exp(-lambda*phi))/lambda, 
     from = 0, to = 1, add = TRUE, col = "red")

# Now the power law results.  We first obtain the MLE estimates of the power
# law exponent
alpha <- igraph::fit_power_law(igraph::degree(g.underreporting))$alpha
# We then create a power law network
g.power_law <- igraph::sample_fitness_pl(
    no.of.nodes = length(igraph::V(g.underreporting)), 
    no.of.edges = length(igraph::E(g.underreporting)), 
    exponent.out = alpha, 
    finite.size.correction = TRUE)
# We then simulate percolation on this network using the above algorithm
S <- numeric(120) # vector to hold our results
sim_size <- 1000 # Number of simulations
i <- numeric(sim_size) # Vector to hold cluster sizes
for (n in 0:119) {
    # n is the number of nodes to remove
    for (m in 1:sim_size) {
        # m is the mth simulation
        to_remove <- sample(1:119, n)
        # randomly pick n of the 119 nodes to remove
        i[m] <- igraph::components(
            igraph::delete_vertices(g.power_law, to_remove))$csize[1]/119
        # calculate the size of the giant component and save the result to
        # the variable i
    }
    S[n + 1] <- mean(i)
    # the average cluster size (after removing n nodes) is the average of the
    # i variable
}

# Let's plot
lines(y = rev(S), x = (0:119)/119, col = "blue")

# Add a legend
legend("bottomleft", 
       legend = c("Simulation", "Poisson", "Power Law"), 
       col = c("black", "red", "blue"), 
       lty = c(1, 1, 1))
```

# Discussion

The graph displays the occupation probability $\phi$ (fraction of vertices
remaining in the graph) vs. the percolation strength $S$ (the average size of
the giant component).  If we start off at the right hand of the graph, we get 
the original (connected) graph.  As we move towards the left, more and more vertices
are removed.  We see that by the time we have removed about half the nodes, the
percolation probability is around 48%, and after removing about 2/3 of the nodes,
the percolation probability is 29%.

Note that for Poisson random graphs, we would expect to see a critical $\phi_c$
above which we get percolation almost surely.  In contrast, for a purely power
law distribution, removing many vertices should still keep the network intact.
In contrast, our graph has a smooth relationship between average cluster size
and occupation probability (with average cluster size declining slightly faster
than the occupation probability).

Note that for both the observed network and power-law network, we do not observe
a sharp "phase transition" at the perclation threshold (indeed, there is no
percolation threshold).  This is a common phenomenon in graphs with power-law (or
truncated power-law) distriubtions--essentially, even as many vertices are removed,
the network continues to function.

# The overreporting model

The above simulation looked at the underreporting model (i.e. we assumed that if
any household reported a link with another, that such a link exists).

We now examine percolation in the overreporting model (we only say a link exists
when both households report the link).  Note that in the overreporting model, the
largest component does not extend over the whole network.

We first (re)create the graph:

```{r overreporting model create graph}
# create the graph (see the basic statistics section for  documentation of the
# procedure below)
overreporting.df <- 
    nyakatoke[nyakatoke$willingness_link1 == 1 & nyakatoke$willingness_link2 == 1,]
g.overreporting <- igraph::graph_from_data_frame(overreporting.df, directed = FALSE)
missing.vertices <- c(7, 30, 32, 46, 44, 65, 84, 88, 91, 96, 107, 110, 
                      116, 117, 118, 119)
g.overreporting <- (Reduce(f = function(x, y) {y + igraph::vertex(x)},
       x = missing.vertices,
       init = g.overreporting,
       right = TRUE))
g.overreporting <- igraph::simplify(g.overreporting)
```

We now perform the same simulation as above, but for the overreporting network:

```{r percolation overreporting, fig.width=6, fig.height=4}
S <- numeric(120) # vector to hold our results
sim_size <- 1000 # Number of simulations
i <- numeric(sim_size) # Vector to hold cluster sizes
for (n in 0:119) {
    # n is the number of nodes to remove
    for (m in 1:sim_size) {
        # m is the mth simulation
        to_remove <- sample(1:119, n)
        # randomly pick n of the 119 nodes to remove
        i[m] <- igraph::components(
            igraph::delete_vertices(g.overreporting, to_remove))$csize[1]/119
        # calculate the size of the giant component and save the result to
        # the variable i
    }
    S[n + 1] <- mean(i)
    # the average cluster size (after removing n nodes) is the average of the 
    # i variable
}
my_df <- data.frame(phi = 1 - (0:119)/119, S = S)
# rbind(my_df, c(phi = 0, S = 1), c(phi = 1, S = 0))
main = expression(paste("Occupaion Probability, ", phi, ", vs. size of giant cluster ", S))
plot(my_df[1:120,], 
     type = "l", 
     main = main,
     sub = "Overreporting Model",
     xlab = expression(phi),
     xlim = c(1,0))
lambda <- mean(igraph::degree(g.overreporting))
plot(function(phi) phi + gsl::lambert_W0(-lambda * phi * exp(-lambda*phi))/lambda, 
     from = 0, to = 1, add = TRUE, col = "red")

# Now the power law results.  We first obtain the MLE estimates of the power
# law exponent
alpha <- igraph::fit_power_law(igraph::degree(g.overreporting))$alpha
# We then create a power law network
g.power_law <- igraph::sample_fitness_pl(
    no.of.nodes = length(igraph::V(g.overreporting)), 
    no.of.edges = length(igraph::E(g.overreporting)), 
    exponent.out = alpha, 
    finite.size.correction = TRUE)
# We then simulate percolation on this network using the above algorithm
S <- numeric(120) # vector to hold our results
sim_size <- 1000 # Number of simulations
i <- numeric(sim_size) # Vector to hold cluster sizes
for (n in 0:119) {
    # n is the number of nodes to remove
    for (m in 1:sim_size) {
        # m is the mth simulation
        to_remove <- sample(1:119, n)
        # randomly pick n of the 119 nodes to remove
        i[m] <- igraph::components(
            igraph::delete_vertices(g.power_law, to_remove))$csize[1]/119
        # calculate the size of the giant component and save the result to
        # the variable i
    }
    S[n + 1] <- mean(i)
    # the average cluster size (after removing n nodes) is the average of the
    # i variable
}

# Let's plot
lines(y = rev(S), x = (0:119)/119, col = "blue")

# Add a legend
legend("bottomleft", 
       legend = c("Simulation", "Poisson", "Power Law"), 
       col = c("black", "red", "blue"), 
       lty = c(1, 1, 1))
```

# Discussion (overreporting model)

As stated earlier, the largest component in the overreporting model does not
cover all the nodes in the graph.  The size of the giant component also appears
to breakdown much faster than in the underreporting model.  When about a quarter
of the nodes are removed from the network, the largest component only connects
half of the households.

# Desire-to-link model
We repeat the analysis for the desire to link model.

```{r desire to link percolation, fig.width=6, fig.height=4}
edgelist <- as.matrix(nyakatoke[nyakatoke$willingness_link1 == 1, 1:2])
g.desire_to_link <- igraph::graph_from_data_frame(edgelist)
S <- numeric(120) # vector to hold our results
sim_size <- 1000 # Number of simulations
i <- numeric(sim_size) # Vector to hold cluster sizes
for (n in 0:119) {
    # n is the number of nodes to remove
    for (m in 1:sim_size) {
        # m is the mth simulation
        to_remove <- sample(1:119, n)
        # randomly pick n of the 119 nodes to remove
        i[m] <- max(igraph::components(
            igraph::delete_vertices(g.desire_to_link, to_remove), mode = "strong")$csize)/119
        # calculate the size of the giant component and save the result to
        # the variable i
    }
    S[n + 1] <- mean(i)
    # the average cluster size (after removing n nodes) is the average of the 
    # i variable
}
my_df <- data.frame(phi = 1 - (0:119)/119, S = S)
# rbind(my_df, c(phi = 0, S = 1), c(phi = 1, S = 0))
main = expression(paste("Occupaion Probability, ", phi, ", vs. size of giant cluster ", S))
plot(my_df[1:120,], 
     type = "l", 
     main = main,
     sub = "Desire-to-link Model",
     xlab = expression(phi),
     xlim = c(1,0))
lambda <- mean(igraph::degree(g.desire_to_link, mode = "in"))
plot(function(phi) phi + gsl::lambert_W0(-lambda * phi * exp(-lambda*phi))/lambda, 
     from = 0, to = 1, add = TRUE, col = "red")

# Now the power law results.  We first obtain the MLE estimates of the power
# law exponent
alpha_1 <- igraph::fit_power_law(igraph::degree(g.desire_to_link, mode = "in"))$alpha
alpha_2 <- igraph::fit_power_law(igraph::degree(g.desire_to_link, mode = "out"))$alpha
# We then create a power law network
g.power_law <- igraph::sample_fitness_pl(
    no.of.nodes = length(igraph::V(g.desire_to_link)), 
    no.of.edges = length(igraph::E(g.desire_to_link)), 
    exponent.in = alpha_1, exponent.out = alpha_2, 
    finite.size.correction = TRUE)
# We then simulate percolation on this network using the above algorithm
S <- numeric(120) # vector to hold our results
sim_size <- 1000 # Number of simulations
i <- numeric(sim_size) # Vector to hold cluster sizes
for (n in 0:119) {
    # n is the number of nodes to remove
    for (m in 1:sim_size) {
        # m is the mth simulation
        to_remove <- sample(1:119, n)
        # randomly pick n of the 119 nodes to remove
        i[m] <- igraph::components(
            igraph::delete_vertices(g.power_law, to_remove))$csize[1]/119
        # calculate the size of the giant component and save the result to
        # the variable i
    }
    S[n + 1] <- mean(i)
    # the average cluster size (after removing n nodes) is the average of the
    # i variable
}

# Let's plot
lines(y = rev(S), x = (0:119)/119, col = "blue")

# Add a legend
legend("bottomleft", 
       legend = c("Simulation", "Poisson", "Power Law"), 
       col = c("black", "red", "blue"), 
       lty = c(1, 1, 1))
```